\section{Theory (50pt)}

\subsection{Energy Based Models Intuition (15pts) }
This question tests your intuitive understanding of Energy-based models and their properties.
\begin{enumerate}[(a)]

    \item (1pts) How do energy-based models allow for modeling situations where the mapping from input $x_i$ to output $y_i$ is not 1 to 1, but 1 to many?



    \item (2pts) How do energy-based models differ from models that output probabilities?

    \item  (2pts) How can you use energy function $F_W(x, y)$ to calculate a probability $p(y \mid x)$?

    \item (2pts) What are the roles of the loss function and energy function?

    \item (2pts) Can loss function be equal to the energy function?

    \item (2pts) What problems can be caused by using only positive examples for energy (pushing down energy of correct inputs only)? How can it be avoided?

    \item
          (2pts) Briefly explain the three methods that can be used to shape the energy function.

    \item (2pts) Provide an example of a loss function that uses negative examples. The format should be as follows $\ell_\text{example}(x, y, W) = F_W(x, y)$.

\end{enumerate}

\textbf{Solution:}

\subsubsection*{(a)}
Energy Based Model can give the energy estimation of different given y, thus for one to many mapping, we can set a threshold to get the y inference list.

\subsubsection*{(b)}
Probability Model is a special case of Energy Based Model.

- The energy is such that the integral $\int_{y \in \mathcal{Y}} e^{-\beta E(W, y, X)}$ (partition function) converges.

- The model is trained by minimizing the negative log-likelihood loss.

\subsubsection*{(c)}
How can you use energy function $F_W(x, y)$ to calculate a probability $p(y \mid x)$?

\begin{equation}
    P(y \mid x)=\frac{e^{-\beta F(x, y)}}{\int_{y^{\prime}} e^{-\beta F\left(x, y^{\prime}\right)}}
\end{equation}

\subsubsection*{(d)}
What are the roles of the loss function and energy function?

The loss function is used to minimizing the energy for target training data points and maximize the energy for the rest of the data points (if they are in the loss).
The energy function is used to calculate the energy of the data points.


\subsubsection*{(e)}
Can loss function be equal to the energy function?

If we only consider the target training data points, the loss function is equal to the energy function. We can say we want low energy/loss in the target points.

For EBM, final optimization (if we call the whole optimization target as   loss) is the optimization about energy of target points and other areas, so they are different.

\subsubsection*{(f)}
What problems can be caused by using only positive examples for energy (pushing down energy of correct inputs only)? How can it be avoided?

The model is not robust, the decision boundary is not that clear, and adversarial attack can ruin the model.

Push up other areas energy. Or contrastively make the energy gap between target points and other area larger.



\subsubsection*{(g)}
Briefly explain the three methods that can be used to shape the energy function.

\textbf{Contrastive methods}:

1. Push down on energy of training samples. Pull up on energy of suitably-generated contrastive samples or anywhere else.

2. Train a function that maps points off the data manifold to points on the data manifold.

\textbf{Regularized Methods}:
1. Regularizer minimizes the volume of space that can take low energy.

2. build the machine so that the volume of low energy space is bounded

3. minimize the gradient and maximize the curvature around data points:

\subsubsection*{(h)}
Provide an example of a loss function that uses negative examples. The format should be as follows $\ell_\text{example}(x, y, W) = F_W(x, y)$.

% \begin{align}
%     \ell_\text{example}(x, y, W) & = F_W(x, y) \\
%                                  & = s
% \end{align}

Hinge loss
\begin{equation}
    \mathcal{L}(x, y, W)=\sum_{\hat{y} \in \mathcal{Y}}\left[F_{W}(x, y)-F_{w}(x, \hat{y})+m(y, \hat{y})\right]^{+}
\end{equation}

% if we use $\hat{y} = f(x)$, and f is our model,

% \begin{equation}
%     \mathcal{L}(x, y, w)=\sum_{\hat{y} \in \mathcal{Y}}\left[F_{w}(x, y)-F_{w}(x, \hat{y})+m(y, \hat{y})\right]^{+}
%     \end{equation}




\subsection{Negative log-likelihood loss (20 pts) }

Let's consider an energy-based model we are training to do classification of input between n classes. $F_W(x, y)$ is the energy of input $x$ and class $y$. We consider n classes: $y \in \{1, \dots, n\}$.

\begin{enumerate}[(i)]
    \item (2pts) For a given input $x$, write down an expression for a Gibbs distribution over labels $y$ that this energy-based model specifies. Use $\beta$ for the constant multiplier.

    \item (5pts) Let's say for a particular data sample $x$, we have the label $y$. Give the expression for the negative log likelihood loss, i.e. negative log likelihood of the correct label (don't copy expressions from the slides, show step-by-step derivation of the loss function from the expression of the previous subproblem). For easier calculations in the following subproblem, multiply the loss by $\frac{1}{\beta}$.

    \item (8pts) Now, derive the gradient of that expression with respect to $W$ (just providing the final expression is not enough). Why can it be intractable to compute it, and how can we get around the intractability?

    \item (5pts) Explain why negative log-likelihood loss pushes the energy of the correct example to negative infinity, and all others to positive infinity, no matter how close the two examples are, resulting in an energy surface with really sharp edges in case of continuous $y$ (this is usually not an issue for discrete $y$ because there's no distance measure between different classes).

\end{enumerate}

\subsubsection*{(i)}
For a given input $x$, write down an expression for a Gibbs distribution over labels $y$ that this energy-based model specifies. Use $\beta$ for the constant multiplier.

% \begin{equation}
%     P_{w}(y)=\frac{e^{-\beta F_{w}(y)}}{\sum_{y^{\prime}} e^{-\beta F_{w}\left(y^{\prime}\right)}}
%     \end{equation}

\begin{align}
    P_{w}(y \mid x)
     & =\frac{e^{-\beta F_{w}(x, y)}}{\int_{y^{\prime}} e^{-\beta F_{w}\left(x, y^{\prime}\right)}} \\
     & =\frac{e^{-\beta F_{w}(x, y)}}
    {\sum_{y^{\prime} \in {1,...n}} e^{-\beta F_{w}\left(x, y^{\prime}\right)}}
\end{align}\label{eq:p_y_given_x}

\subsubsection*{(ii)}
Let's say for a particular data sample $x$, we have the label $y$. Give the expression for the negative log likelihood loss, i.e. negative log likelihood of the correct label (don't copy expressions from the slides, show step-by-step derivation of the loss function from the expression of the previous subproblem). For easier calculations in the following subproblem, multiply the loss by $\frac{1}{\beta}$.

We have \eqref{eq:p_y_given_x}, according to the negative log likelihood loss definition,

\begin{align}
    \mathcal{L}(x, y, w)
     & = -\frac{1}{\beta} \log P_{w}(y \mid x)                                                                                          &  & (\text{multiply $\frac{1}{\beta}$ for convenience}) \\
     & = -\frac{1}{\beta} \log \frac{e^{-\beta F_{w}(x, y)}}{\sum_{y^{\prime} \in {1,...n}} e^{-\beta F_{w}\left(x, y^{\prime}\right)}}                                                          \\
     & = F_{w}(x, y) +
    \frac{1}{\beta} \log
    \underbrace{
        \left[
            \sum_{y^{\prime} \in {1,...n}} e^{-\beta F_{w}\left(x, y^{\prime}\right)}
            \right]}_{P}
    \label{eq:loss}
\end{align}

The first term is the energy of the correct label, the second term is the log of the sum of the energy of all other labels. we would like to minimize the correct label energy and free energy over y.


\subsubsection*{(iii)}
Now, derive the gradient of that expression with respect to $W$ (just providing the final expression is not enough). Why can it be intractable to compute it, and how can we get around the intractability?


\begin{align}
    \frac{\partial \mathcal{L}(x, y, w)}{\partial w}
     & = \frac{\partial F_w(x, y)}{\partial w}
    + \frac{1}{\beta}
    \frac{\partial \log  \left[
            \sum_{y^{\prime} \in {1,...n}} e^{-\beta F_{w}\left(x, y^{\prime}\right)}
    \right] }{\partial w}                                                                                                       \\
     & = \frac{\partial F_w(x, y)}{\partial w}
    + \frac{1}{\beta}
    \frac{\partial \log P}{\partial P} \sum_{y^{\prime}} \frac{\partial P}{\partial F(x,y')}\frac{\partial F(x,y')}{\partial w} \\
     & = \frac{\partial F_w(x, y)}{\partial w}
    + \frac{1}{\beta}
    \frac{1}{P} (-\beta \sum_{y^{\prime} \in \{1 \ldots n\}} e^{-\beta F_{w}\left(x, y^{\prime}\right)})
    \frac{\partial F(x,y')}{\partial w}                                                                                         \\
     & = \frac{\partial F_w(x, y)}{\partial w}
    - \sum_{y^{\prime} \in \{1 \ldots n\}}
    \frac{e^{-\beta F_{w}(x, y)}}
    {\sum_{y^{\prime}} e^{-\beta F_{w}\left(x, y^{\prime}\right)}}
    {\partial F_{w}\left(x, y^{\prime}\right)}{\partial w}                                                                      \\
     & = \frac{\partial F_w(x, y)}{\partial w}
    - \sum_{y^{\prime}} P_{w}\left(y^{\prime} \mid x\right) \frac{\partial F_{w}\left(x, y^{\prime}\right)}{\partial w}
\end{align}

In the discrete classification case, we need to calculate all the energy of all the combination of x and possible label y, which is intractable. We can use Monte Carlo Methods to sample y from P(y|x) to approximate the integral.


\subsection{Comparing Contrastive Loss Functions (15pts)}

In this problem, we're going to compare a few contrastive loss functions. We are going to look at the behavior of the gradients, and understand what uses each loss function has. In the following subproblems, $m$ is a margin, $m \in \R$, $x$ is input, $y$ is the correct label, $\bar y$ is the incorrect label. Define the loss in the following format: $\ell_{example}(x, y, \bar y, W) = F_W(x, y)$.

\begin{enumerate}[(a)]
    \item (3pts) \textbf{Simple loss function} is defined as follows:

          $$
              \ell_\text{simple}(x, y, \bar y, W) = \left[ F_W(x, y)\right]^+ + \left[m - F_W(x, \bar y)\right]^+
          $$

          Assuming we know the derivative $\pdv{F_W(x, y)}{W}$ for any $x, y$, give an expression for the partial derivative of the $\ell_\text{simple}$ with respect to $W$.

    \item (3pts) \textbf{Hinge loss} is defined as follows:

          $$
              \ell_\text{hinge}(x, y, \bar y, W) = \left[ F_W(x, y) - F_W(x, \bar y) + m\right ]^+
          $$

          Assuming we know the derivative $\pdv{F_W(x, y)}{W}$ for any $x, y$, give an expression for the partial derivative of the $\ell_\text{hinge}$ with respect to $W$.

    \item (3pts) \textbf{Square-Square loss} is defined as follows:

          $$
              \ell_\text{square-square}(x, y, \bar y, W) = \left(\left[ F_W(x, y)\right]^+ \right)^2 + \left( \left[m - F_W(x, \bar y)\right]^+ \right)^2
          $$

          Assuming we know the derivative $\pdv{F_W(x, y)}{W}$ for any $x, y$, give an expression for the partial derivative of the $\ell_\text{square-square}$ with respect to $W$.

    \item (6pts) \textbf{Comparison.}
          \begin{enumerate}[(i)]
              \item (2pts) Explain how NLL loss is different from the three losses above.
              \item (2pts) What is the role of the margin in hinge loss? Why do we take only the positive part of $ F_W(x, y) - F_W(x, \bar y) + m$?
              \item (2pts) How are simple loss and square-square loss different from hinge loss? In what situations would you use simple loss, and in what situations would you use square-square loss?
          \end{enumerate}

\end{enumerate}


\subsubsection*{(a)}


$$
    \ell_\text{simple}(x, y, \bar y, W) = \left[ F_W(x, y)\right]^+ + \left[m - F_W(x, \bar y)\right]^+
$$

\begin{align}
    \frac{\partial \ell_\text{simple}(x, y, \bar y, W)}{\partial W}
     & = \frac{\partial \left[F_W(x, y)\right]^+}{\partial W}
    + \frac{\partial \left[m -F_W(x, \bar y)\right]^+}{\partial W}
\end{align}

\begin{align}
    \frac{\partial \left[F_W(x, y)\right]^+}{\partial W} =
    \begin{cases}
        \frac{\partial F_W(x, y)}{\partial W} & \text{if } F_W(x, y) > 0    \\
        0                                     & \text{if } F_W(x, y) \leq 0 \\
    \end{cases}
\end{align}

\begin{align}
    \frac{\partial \left[m -F_W(x, \bar y)\right]^+}{\partial W} =
    \begin{cases}
        - \frac{\partial F_W(x, \bar y)}{\partial W} & \text{if } F_W(x, \bar y) < m    \\
        0                                            & \text{if } F_W(x, \bar y) \geq m \\
    \end{cases}
\end{align}

\subsubsection*{(b)}
$$
    \ell_\text{hinge}(x, y, \bar y, W) = \left[ F_W(x, y) - F_W(x, \bar y) + m\right ]^+
$$

\begin{align}
    \frac{\partial \ell_\text{hinge}(x, y, \bar y, W)}{\partial W}
     & = \frac{\partial \left[F_W(x, y) - F_W(x, \bar y) + m\right]^+}{\partial W}                                                             \\
     & = \begin{cases}
             \frac{\partial F_W(x, y)}{\partial W} - \frac{\partial F_W(x, \bar y)}{\partial W} & \text{if } F_W(x, y) - F_W(x, \bar y) + m >0     \\
             0                                                                                  & \text{if } F_W(x, y) - F_W(x, \bar y) + m \leq 0 \\
         \end{cases}
\end{align}


\subsubsection*{(c)}
$$
    \ell_\text{square-square}(x, y, \bar y, W) = \left(\left[ F_W(x, y)\right]^+ \right)^2 + \left( \left[m - F_W(x, \bar y)\right]^+ \right)^2
$$

\begin{align}
    \frac{\partial \ell_\text{square-square}(x, y, \bar y, W)}{\partial W}
     & = 2 \left[F_W(x, y)\right]^+
    \frac{\partial \left[F_W(x, y)\right]^+}{\partial W}
    + 2 \left[m -F_W(x, \bar y)\right]^+
    \frac{\partial \left[m -F_W(x, \bar y)\right]^+}{\partial W}                                                                                                                                                                       \\
     & = \begin{cases}
             2 \left[F_W(x, y)\right] \frac{\partial \left[F_W(x, y)\right]}{\partial W}                                                                               & \text{if } F_W(x, y) > 0  \text{ and }  F_W(x, \bar{y}) > m       \\
             2 \left[F_W(x, y)\right] \frac{\partial \left[F_W(x, y)\right]}{\partial W} - 2 \left[m -F_W(x, \bar y)\right] \frac{\partial F_W(x, \bar y)}{\partial W} & \text{if } F_W(x, y) > 0 \text{ and }  F_W(x, \bar{y}) \leq  m   \\
             0                                                                                                                                                         & \text{if } F_W(x, y) \leq 0  \text{ and }  F_W(x, \bar{y}) >  m   \\
             - 2 \left[m -F_W(x, \bar y)\right] \frac{\partial F_W(x, \bar y)}{\partial W}                                                                             & \text{if } F_W(x, y) \leq 0  \text{ and }  F_W(x, \bar{y}) \leq m \\
         \end{cases}
\end{align}



% \item (6pts) \textbf{Comparison.}
% \begin{enumerate}[(i)]
%     \item (2pts) Explain how NLL loss is different from the three losses above.
%     \item (2pts) What is the role of the margin in hinge loss? Why do we take only the positive part of $ F_W(x, y) - F_W(x, \bar y) + m$?
%     \item (2pts) How are simple loss and square-square loss different from hinge loss? In what situations would you use simple loss, and in what situations would you use square-square loss?
% \end{enumerate}

\subsubsection*{(d)}
\textbf{(i)}

NLI loss is not pair-wise contrastive, it try to minimize the energy of target energy and the free energy of y.

The three pair wise loss try to maxmize the energy gap between target and other location energy.

\textbf{(ii)}

The margin term is the gap between the target (training data point) energy and the other location energy. For the negative part, $F_W(x, y) - F_W(x, \bar y) + m <0$, which means $F_W(x, y) - F_W(x, \bar y) <m$, the gap between the target and the contrastive negative energy is larger than the margin, we stop to optimize the energy gap.

\textbf{(iii)}

\textbf{Optimization}
Just like L1 and L2 norm, the optimization of square-square loss is more efficient in non-spase case. We will use square-square loss in non-spase loss calculation. In the meanwhile, square-square loss solution space is more stable, as illustrated below.

\textbf{Relative Gap and Absolute Value}
The simple loss and hinge loss are two-piecewise loss, the threshold is based on the relative energy gap of the target and the other location, but square-square loss can be utilized to force the target energy less than zero and the other location energy larger than m. The square-square loss do not only optimize the relative energy gap, but also optimize the absolute energy range.

If we only would like to optimize the relative energy gap, we can use the simple loss, but if we want to control the absolute energy range, we can use the square-square loss.
