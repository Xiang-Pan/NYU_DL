{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtcBjMq7YV3f"
   },
   "source": [
    "\n",
    "\n",
    "# Homework 2 - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn-cOk1iZTtR"
   },
   "source": [
    "In this part of the homework we are going to work with Recurrent Neural Networks, in particular GRU. One of the greatest things that Recurrent Neural Networks can do when working with sequences is retaining data from several timesteps in the past. We are going to explore that property by constructing an 'echo' Recurrent Neural Network.\n",
    "\n",
    "The goal here is to make a model that given a sequence of letters or digits will output that same sequence, but with a certain delay. Let's say the input is a string 'abacaba', we want the model to not output anything for 3 steps (delay length), and then output the original string step by step, except the last 3 characters. So, target output is then 'XXXabac', where 'X' is empty output.\n",
    "\n",
    "This is similar to [this notebook](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb) (which you should refer to when doing this assignment), except we're working not with a binary string, but with a sequence of integers between 0 and some N. In our case N is 26, which is the number of letters in the alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npLlE973as6x"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Let's implement the dataset. In our case, the data is basically infinite, as we can always generate more examples on the fly, so there's no need to load it from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Max value of the generated integer. 26 is chosen becuase it's\n",
    "# the number of letters in English alphabet.\n",
    "N = 26\n",
    "\n",
    "\n",
    "def idx_to_onehot(x, k=N+1):\n",
    "    \"\"\" Converts the generated integers to one-hot vectors \"\"\"\n",
    "    ones = torch.sparse.torch.eye(k)\n",
    "    shape = x.shape\n",
    "    res = ones.index_select(0, x.view(-1).type(torch.int64))\n",
    "    return res.view(*shape, res.shape[-1])\n",
    "\n",
    "def s_to_idx(s):\n",
    "    \"\"\" Converts a string to a list of integers \"\"\"\n",
    "    return [(ord(c) - ord('a') +1) for c in s]\n",
    "\n",
    "def idx_to_s(idx):\n",
    "    \"\"\" Converts a list of integers to a string \"\"\"\n",
    "    return ''.join([chr(c + ord('a') - 1) for c in idx])\n",
    "\n",
    "\n",
    "class EchoDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(self, delay=4, seq_length=15, size=1000):\n",
    "        self.delay = delay\n",
    "        self.seq_length = seq_length\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Iterable dataset doesn't have to implement __getitem__.\n",
    "            Instead, we only need to implement __iter__ to return\n",
    "            an iterator (or generator).\n",
    "        \"\"\"\n",
    "        for _ in range(self.size):\n",
    "            seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
    "            result = torch.cat((torch.zeros(self.delay), seq[:self.seq_length - self.delay])).type(torch.int64)\n",
    "            yield seq, result\n",
    "\n",
    "DELAY = 4\n",
    "DATASET_SIZE = 200000\n",
    "ds = EchoDataset(delay=DELAY, size=DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNrZqYURcKSl"
   },
   "source": [
    "## Model\n",
    "\n",
    "Now, we want to implement the model. For our purposes, we want to use GRU. The architecture consists of GRU and a decoder. Decoder is responsible for decoding the GRU hidden state to yield a predicting for the next output. The parts you are responsible for filling with your code are marked with `TODO`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class GRUMemory(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        #TODO: initialize your submodules\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.embedding_size = 10\n",
    "        # self.embedding = torch.nn.Embedding(N+1, self.embedding_size)\n",
    "        \n",
    "        \n",
    "        # do not use embedding here\n",
    "        self.gru = torch.nn.GRU(N+1,\n",
    "                                self.hidden_size, \n",
    "                                num_layers=1,\n",
    "                                batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, N+1)\n",
    "        self.hidden = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # inputs: x - input tensor of shape (batch_size, seq_length, N+1)\n",
    "        # returns:\n",
    "        # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n",
    "        embedding = x\n",
    "        gru_outs, _ = self.gru(embedding)\n",
    "        logits = self.linear(gru_outs)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_run(self, s):\n",
    "        # This function accepts one string s containing lowercase characters a-z. \n",
    "        # You need to map those characters to one-hot encodings, \n",
    "        # then get the result from your network, and then convert the output \n",
    "        # back to a string of the same length, with 0 mapped to ' ', \n",
    "        # and 1-26 mapped to a-z.\n",
    "        idx = s_to_idx(s)\n",
    "        idx = torch.tensor(idx, dtype=torch.int64)\n",
    "        onehot = idx_to_onehot(torch.tensor(idx, dtype=torch.int64)).unsqueeze(0)\n",
    "        logits = self.forward(onehot)\n",
    "        pred = torch.argmax(logits, dim=2)\n",
    "        pred = pred.squeeze(0)\n",
    "        pred_str = ''.join([chr(c + ord('a') - 1) for c in pred])\n",
    "        return pred_str\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    # def test_run_tensor(self, s_tensor):\n",
    "    #     # This function accepts one string s containing lowercase characters a-z. \n",
    "    #     # You need to map those characters to one-hot encodings, \n",
    "    #     # then get the result from your network, and then convert the output \n",
    "    #     # back to a string of the same length, with 0 mapped to ' ', \n",
    "    #     # and 1-26 mapped to a-z.\n",
    "        \n",
    "    #     # s_tensor = s_tensor.view(1, len(s_tensor), N+1)\n",
    "    #     logits = self.forward(s_tensor)\n",
    "    #     pred = torch.argmax(logits, dim=2)\n",
    "    #     pred = pred.squeeze(0)\n",
    "    #     pred_str = ''.join([chr(c + ord('a')) for c in pred])\n",
    "    #     return pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619588/257215062.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  onehot = idx_to_onehot(torch.tensor(idx, dtype=torch.int64)).unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'swwwc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test run\n",
    "model = GRUMemory(hidden_size=10)\n",
    "model.test_run('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model == model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9whwmVu9OIx"
   },
   "source": [
    "## Training\n",
    "Below you need to implement the training of the model. We give you more freedom as for the implementation. The two limitations are that it has to execute within 10 minutes, and that error rate should be below 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 4\n",
    "def test_model(model, sequence_length=15):\n",
    "    \"\"\"\n",
    "    This is the test function that runs 100 different strings through your model,\n",
    "    and checks the error rate.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(500):\n",
    "        s = ''.join([random.choice(string.ascii_lowercase) for i in range(random.randint(15, 25))])\n",
    "        result = model.test_run(s)\n",
    "        for c1, c2 in zip(s[:-D], result[D:]):\n",
    "            correct += int(c1 == c2)\n",
    "        total += len(s) - D\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELAY = 4\n",
    "DATASET_SIZE = 200000\n",
    "ds = EchoDataset(delay=DELAY, size=DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9lV9BscxCCAI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_605490/257215062.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  onehot = idx_to_onehot(torch.tensor(idx, dtype=torch.int64)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.9997499687460932 loss: 0.0009174463921226561\n",
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = GRUMemory(hidden_size=128).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[10, 30], gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# TODO\n",
    "epoch = 0\n",
    "test_acc = 0\n",
    "while test_acc <= 0.99:\n",
    "    ds = EchoDataset(delay=DELAY, size=DATASET_SIZE)\n",
    "    train_dataloader = torch.utils.data.DataLoader(ds, batch_size=512)\n",
    "    for x, y in train_dataloader:\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        x_onehot = idx_to_onehot(x)\n",
    "        y_onehot = idx_to_onehot(y)\n",
    "\n",
    "        logits = model(x_onehot)\n",
    "        logits = logits[:, 4:, :]\n",
    "        y = y[:, 4:]\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, N + 1), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "\n",
    "    test_acc = test_model(model)\n",
    "    scheduler.step()\n",
    "    print(f'epoch {epoch}: {test_acc}', f'loss: {loss.item()}')\n",
    "    epoch += 1\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "accuracy = test_model(model)\n",
    "assert duration < 600, 'execution took f{duration:.2f} seconds, which longer than 10 mins'\n",
    "assert accuracy > 0.99, f'accuracy is too low, got {accuracy}, need 0.99'\n",
    "print('tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB0EVNBtDhpN"
   },
   "source": [
    "## Variable delay model\n",
    "\n",
    "Now, to make this more complicated, we want to have varialbe delay. So, now, the goal is to transform a sequence of pairs (character, delay) into a character sequence with given delay. Delay is constant within one sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i_iwX_AEOCH"
   },
   "source": [
    "### Dataset\n",
    "As before, we first implement the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E4G5b8kuEUEd"
   },
   "outputs": [],
   "source": [
    "class VariableDelayEchoDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, max_delay=8, seq_length=20, size=1000):\n",
    "        self.max_delay = max_delay\n",
    "        self.seq_length = seq_length\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.size):\n",
    "            seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
    "            delay = random.randint(0, self.max_delay)\n",
    "            result = torch.cat((torch.zeros(delay), seq[:self.seq_length - delay])).type(torch.int64)\n",
    "            yield seq, delay, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25,  3, 20,  3,  3,  9,  2, 24, 13, 20, 19, 25, 12,  8, 26,  1, 13, 25,\n",
      "        20, 11]) 3 tensor([ 0,  0,  0, 25,  3, 20,  3,  3,  9,  2, 24, 13, 20, 19, 25, 12,  8, 26,\n",
      "         1, 13])\n"
     ]
    }
   ],
   "source": [
    "dataset = VariableDelayEchoDataset(max_delay=8, seq_length=20, size=1000)\n",
    "for seq, delay, result in dataset:\n",
    "    print(seq, delay, result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTRVOND3HEJZ"
   },
   "source": [
    "### Model\n",
    "\n",
    "And the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IYolFIB8Hg0U"
   },
   "outputs": [],
   "source": [
    "class VariableDelayGRUMemory(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, max_delay):\n",
    "        super().__init__()\n",
    "        self.max_delay = max_delay\n",
    "        self.delay_embedding = torch.nn.Embedding(max_delay + 1, hidden_size//2)\n",
    "        self.embedding = torch.nn.Embedding(N + 1, hidden_size//2)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, N + 1)\n",
    "        \n",
    "    def forward(self, x, delays):\n",
    "        # inputs:\n",
    "        # x - tensor of shape (batch size, seq length, N + 1)\n",
    "        # delays - tensor of shape (batch size)\n",
    "        # returns:\n",
    "        # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n",
    "        # (batch size, seq length, N + 1)\n",
    "        embedding = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        delay_embedding = self.delay_embedding(delays).unsqueeze(1)\n",
    "        delay_embedding = delay_embedding.repeat(1, embedding.shape[1], 1)\n",
    "        \n",
    "        \n",
    "        # print(delay_embedding.shape, embedding.shape)\n",
    "        final_embedding = torch.concat((delay_embedding, embedding), dim=2)\n",
    "        # final_embedding = embedding + delay_embedding\n",
    "        output, _ = self.gru(final_embedding)\n",
    "        logits = self.linear(output)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_run(self, s, delay):\n",
    "        # This function accepts one string s containing lowercase characters a-z, \n",
    "        # and a delay - the desired output delay.\n",
    "        # You need to map those characters to one-hot encodings, \n",
    "        # then get the result from your network, and then convert the output \n",
    "        # back to a string of the same length, with 0 mapped to ' ', \n",
    "        # and 1-26 mapped to a-z.\n",
    "\n",
    "        idx = s_to_idx(s)\n",
    "        idx = torch.tensor(idx, dtype=torch.int64)\n",
    "        idx = idx.unsqueeze(0)\n",
    "        delay = torch.tensor(delay, dtype=torch.int64).unsqueeze(0)\n",
    "        # idx = idx.view(1, len(idx), N+1)\n",
    "        logits = self.forward(idx, delay)\n",
    "        pred = torch.argmax(logits, dim=2)\n",
    "        pred = pred.squeeze(0)\n",
    "        pred_str = ''.join([chr(c + ord('a') - 1) for c in pred])\n",
    "        return pred_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riu3qHWgKjsx"
   },
   "source": [
    "### Train\n",
    "\n",
    "As before, you're free to do what you want, as long as training finishes within 10 minutes and accuracy is above 0.99 for delays between 0 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4FZHojnGO3aw"
   },
   "outputs": [],
   "source": [
    "def test_variable_delay_model(model, seq_length=20):\n",
    "  \"\"\"\n",
    "  This is the test function that runs 100 different strings through your model,\n",
    "  and checks the error rate.\n",
    "  \"\"\"\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  for i in range(500):\n",
    "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(seq_length)])\n",
    "    d = random.randint(0, model.max_delay)\n",
    "    result = model.test_run(s, d)\n",
    "    if d > 0:\n",
    "      z = zip(s[:-d], result[d:])\n",
    "    else:\n",
    "      z = zip(s, result)\n",
    "    for c1, c2 in z:\n",
    "      correct += int(c1 == c2)\n",
    "    total += len(s) - d\n",
    "\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YJ18Ef6vKi4s",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 0.2729652249781877 loss: 1.6668933629989624\n",
      "epoch 1: 0.4232469993682881 loss: 1.2804588079452515\n",
      "epoch 2: 0.50125 loss: 0.8192830085754395\n",
      "epoch 3: 0.5381036217303823 loss: 0.9128271341323853\n",
      "epoch 4: 0.5881170707197588 loss: 0.7976838946342468\n",
      "epoch 5: 0.6471746891542534 loss: 0.7809659242630005\n",
      "epoch 6: 0.6696793363872725 loss: 0.7855949997901917\n",
      "epoch 7: 0.6588368295440271 loss: 0.8515223860740662\n",
      "epoch 8: 0.6766527619769941 loss: 0.7032785415649414\n",
      "epoch 9: 0.7011335012594458 loss: 0.7776274681091309\n",
      "epoch 10: 0.7268970698722765 loss: 0.5595321655273438\n",
      "epoch 11: 0.7382666997765086 loss: 0.587973415851593\n",
      "epoch 12: 0.7606000495908752 loss: 0.5595741271972656\n",
      "epoch 13: 0.7769249814310473 loss: 0.5864611268043518\n",
      "epoch 14: 0.7705368289637953 loss: 0.34679415822029114\n",
      "epoch 15: 0.7708647561588738 loss: 0.3703606128692627\n",
      "epoch 16: 0.7685196910219071 loss: 0.3569595515727997\n",
      "epoch 17: 0.7974191931846655 loss: 0.4658064842224121\n",
      "epoch 18: 0.7938921704159859 loss: 0.3445974290370941\n",
      "epoch 19: 0.8075093867334168 loss: 0.36369776725769043\n",
      "epoch 20: 0.8154404404404404 loss: 0.4557628631591797\n",
      "epoch 21: 0.82392439691619 loss: 0.17853225767612457\n",
      "epoch 22: 0.8338073451218004 loss: 0.3438650667667389\n",
      "epoch 23: 0.8227289908025702 loss: 0.2277088165283203\n",
      "epoch 24: 0.8359940134696932 loss: 0.5037515759468079\n",
      "epoch 25: 0.8318273092369478 loss: 0.27010378241539\n",
      "epoch 26: 0.8349127182044888 loss: 0.3066509962081909\n",
      "epoch 27: 0.8498382682259269 loss: 0.31162601709365845\n",
      "epoch 28: 0.8397783096107823 loss: 0.2997949719429016\n",
      "epoch 29: 0.8627183744269608 loss: 0.2841017246246338\n",
      "epoch 30: 0.8564485981308412 loss: 0.21918483078479767\n",
      "epoch 31: 0.8682440846824409 loss: 0.18806374073028564\n",
      "epoch 32: 0.8692997757288812 loss: 0.27074578404426575\n",
      "epoch 33: 0.8579552634904587 loss: 0.34197038412094116\n",
      "epoch 34: 0.8723030607124937 loss: 0.2850659191608429\n",
      "epoch 35: 0.8758914049793569 loss: 0.27635160088539124\n",
      "epoch 36: 0.8732200849362978 loss: 0.26540207862854004\n",
      "epoch 37: 0.8868626721250465 loss: 0.17342714965343475\n",
      "epoch 38: 0.8732021196063589 loss: 0.2631067931652069\n",
      "epoch 39: 0.8803482587064677 loss: 0.19528569281101227\n",
      "epoch 40: 0.8967606330365975 loss: 0.19851739704608917\n",
      "epoch 41: 0.8889167502507522 loss: 0.3162911534309387\n",
      "epoch 42: 0.89369931378665 loss: 0.2136639803647995\n",
      "epoch 43: 0.8996500874781305 loss: 0.10948453098535538\n",
      "epoch 44: 0.8996990972918756 loss: 0.24424491822719574\n",
      "epoch 45: 0.9120019858508129 loss: 0.1791168749332428\n",
      "epoch 46: 0.9002396266868458 loss: 0.3139267861843109\n",
      "epoch 47: 0.9058206345241069 loss: 0.14985136687755585\n",
      "epoch 48: 0.9052816459666291 loss: 0.26634302735328674\n",
      "epoch 49: 0.9094768385566238 loss: 0.20764277875423431\n",
      "epoch 50: 0.9051178109956929 loss: 0.26396214962005615\n",
      "epoch 51: 0.9121848217612719 loss: 0.2532249391078949\n",
      "epoch 52: 0.9097441068952477 loss: 0.08191431313753128\n",
      "epoch 53: 0.9136772721510965 loss: 0.27696463465690613\n",
      "epoch 54: 0.9150187734668336 loss: 0.14360137283802032\n",
      "epoch 55: 0.9327431452108693 loss: 0.06996187567710876\n",
      "epoch 56: 0.9247609461499748 loss: 0.13297973573207855\n",
      "epoch 57: 0.9192678951986962 loss: 0.09608898311853409\n",
      "epoch 58: 0.9226017899911761 loss: 0.16332054138183594\n",
      "epoch 59: 0.9273451107240607 loss: 0.07140788435935974\n",
      "epoch 60: 0.9350520575111552 loss: 0.31192630529403687\n",
      "epoch 61: 0.9261871772263509 loss: 0.15473680198192596\n",
      "epoch 62: 0.9413652635477101 loss: 0.07967743277549744\n",
      "epoch 63: 0.9364140015109544 loss: 0.1941240429878235\n",
      "epoch 64: 0.9274761186525893 loss: 0.11681016534566879\n",
      "epoch 65: 0.9362260847817931 loss: 0.2222389280796051\n",
      "epoch 66: 0.9353015075376885 loss: 0.158159539103508\n",
      "epoch 67: 0.9340797584601837 loss: 0.20932970941066742\n",
      "epoch 68: 0.9356981699674104 loss: 0.14353567361831665\n",
      "epoch 69: 0.94325 loss: 0.14842604100704193\n",
      "epoch 70: 0.9389712292938099 loss: 0.12411562353372574\n",
      "epoch 71: 0.9489631193344096 loss: 0.06366514414548874\n",
      "epoch 72: 0.9470964566929134 loss: 0.12452244758605957\n",
      "epoch 73: 0.9423292972163275 loss: 0.12362327426671982\n",
      "epoch 74: 0.9446180772596574 loss: 0.05686724931001663\n",
      "epoch 75: 0.9523573200992556 loss: 0.238406702876091\n",
      "epoch 76: 0.9505806770447245 loss: 0.14753374457359314\n",
      "epoch 77: 0.9457197209892201 loss: 0.16629724204540253\n",
      "epoch 78: 0.9466165413533835 loss: 0.1453271359205246\n",
      "epoch 79: 0.9486537257357546 loss: 0.14667604863643646\n",
      "epoch 80: 0.9437862364507185 loss: 0.08471589535474777\n",
      "epoch 81: 0.9473816400853949 loss: 0.08818664401769638\n",
      "epoch 82: 0.9578328328328328 loss: 0.07345791906118393\n",
      "epoch 83: 0.9611111111111111 loss: 0.07951045036315918\n",
      "epoch 84: 0.9534591194968554 loss: 0.0978187844157219\n",
      "epoch 85: 0.9590716246568505 loss: 0.1038181409239769\n",
      "epoch 86: 0.9566466766616691 loss: 0.10614500194787979\n",
      "epoch 87: 0.9610617253036183 loss: 0.14940162003040314\n",
      "epoch 88: 0.9574122642687648 loss: 0.0963502749800682\n",
      "epoch 89: 0.9627252530239447 loss: 0.09839747101068497\n",
      "epoch 90: 0.9570229294574615 loss: 0.08660894632339478\n",
      "epoch 91: 0.9616336633663366 loss: 0.07952796667814255\n",
      "epoch 92: 0.9612015018773467 loss: 0.06730098277330399\n",
      "epoch 93: 0.9582082082082082 loss: 0.20355649292469025\n",
      "epoch 94: 0.9580367178718622 loss: 0.06902626901865005\n",
      "epoch 95: 0.9657432599554786 loss: 0.045452412217855453\n",
      "epoch 96: 0.9620363455314912 loss: 0.07078122347593307\n",
      "epoch 97: 0.969278223318939 loss: 0.1408199667930603\n",
      "epoch 98: 0.9589782307789103 loss: 0.06563683599233627\n",
      "epoch 99: 0.9588884659307194 loss: 0.07081003487110138\n",
      "epoch 100: 0.9630889392794155 loss: 0.07422119379043579\n",
      "epoch 101: 0.9593037816178313 loss: 0.08743827790021896\n",
      "epoch 102: 0.967881408276714 loss: 0.06808320432901382\n",
      "epoch 103: 0.9642365887207703 loss: 0.07405389845371246\n",
      "epoch 104: 0.9657214870825457 loss: 0.06960609555244446\n",
      "epoch 105: 0.9660763282614119 loss: 0.03419472277164459\n",
      "epoch 106: 0.9660420072946799 loss: 0.17264196276664734\n",
      "epoch 107: 0.9677661169415293 loss: 0.05821242928504944\n",
      "epoch 108: 0.9657293497363796 loss: 0.10349921137094498\n",
      "epoch 109: 0.9713603818615751 loss: 0.0377713106572628\n",
      "epoch 110: 0.9677176234141439 loss: 0.021641109138727188\n",
      "epoch 111: 0.9623144271031595 loss: 0.08206360787153244\n",
      "epoch 112: 0.9727059404717797 loss: 0.09041840583086014\n",
      "epoch 113: 0.9734502191609268 loss: 0.0785149484872818\n",
      "epoch 114: 0.97024256064016 loss: 0.06253176182508469\n",
      "epoch 115: 0.9710822073494128 loss: 0.09085186570882797\n",
      "epoch 116: 0.9673309788092835 loss: 0.040476445108652115\n",
      "epoch 117: 0.9685486926866237 loss: 0.06653722375631332\n",
      "epoch 118: 0.9709295071740487 loss: 0.16854260861873627\n",
      "epoch 119: 0.9774333539987601 loss: 0.10170312970876694\n",
      "epoch 120: 0.9711887761493173 loss: 0.020782163366675377\n",
      "epoch 121: 0.9724770642201835 loss: 0.10452907532453537\n",
      "epoch 122: 0.9762199653207828 loss: 0.04114624857902527\n",
      "epoch 123: 0.9685257459398212 loss: 0.06800428777933121\n",
      "epoch 124: 0.9687813440320963 loss: 0.04968839883804321\n",
      "epoch 125: 0.9767673002857498 loss: 0.08697935193777084\n",
      "epoch 126: 0.9749654044533904 loss: 0.13580389320850372\n",
      "epoch 127: 0.9663443425844531 loss: 0.04381575062870979\n",
      "epoch 128: 0.9787841191066997 loss: 0.004757394548505545\n",
      "epoch 129: 0.9728609121748963 loss: 0.10925867408514023\n",
      "epoch 130: 0.9744968121015127 loss: 0.048421427607536316\n",
      "epoch 131: 0.9725548902195609 loss: 0.022328855469822884\n",
      "epoch 132: 0.9770401796855502 loss: 0.05259820446372032\n",
      "epoch 133: 0.9766169154228855 loss: 0.10989538580179214\n",
      "epoch 134: 0.9783662812383439 loss: 0.06957151740789413\n",
      "epoch 135: 0.9695221372130942 loss: 0.01388753391802311\n",
      "epoch 136: 0.9713960258195166 loss: 0.07517808675765991\n",
      "epoch 137: 0.9732424586646472 loss: 0.05255782604217529\n",
      "epoch 138: 0.9781304673831542 loss: 0.03700099512934685\n",
      "epoch 139: 0.976205307088576 loss: 0.0476534329354763\n",
      "epoch 140: 0.9789355602642403 loss: 0.02278626523911953\n",
      "epoch 141: 0.9747729566094854 loss: 0.07281362265348434\n",
      "epoch 142: 0.9814303638644919 loss: 0.0756874680519104\n",
      "epoch 143: 0.9788451966152315 loss: 0.06073378399014473\n",
      "epoch 144: 0.9769843244588206 loss: 0.018502971157431602\n",
      "epoch 145: 0.974 loss: 0.042790092527866364\n",
      "epoch 146: 0.9778744561839652 loss: 0.04966292902827263\n",
      "epoch 147: 0.9854626093384256 loss: 0.02976551465690136\n",
      "epoch 148: 0.9797119599248592 loss: 0.04925767704844475\n",
      "epoch 149: 0.9769782362561329 loss: 0.04393340274691582\n",
      "epoch 150: 0.9782525753529188 loss: 0.072456493973732\n",
      "epoch 151: 0.9832980181976817 loss: 0.014207976870238781\n",
      "epoch 152: 0.980936954896586 loss: 0.023774849250912666\n",
      "epoch 153: 0.9785867237687366 loss: 0.06985113769769669\n",
      "epoch 154: 0.9829445698520191 loss: 0.12193033844232559\n",
      "epoch 155: 0.9814768771755346 loss: 0.0713564082980156\n",
      "epoch 156: 0.9820917796293993 loss: 0.06203097850084305\n",
      "epoch 157: 0.9766938359653592 loss: 0.030532747507095337\n",
      "epoch 158: 0.9797112272840428 loss: 0.04129266366362572\n",
      "epoch 159: 0.9760771543086172 loss: 0.07870616763830185\n",
      "epoch 160: 0.9799121155053359 loss: 0.04558008536696434\n",
      "epoch 161: 0.9800429455601869 loss: 0.056717630475759506\n",
      "epoch 162: 0.982010315762989 loss: 0.04105532541871071\n",
      "epoch 163: 0.9801867743563857 loss: 0.03440527245402336\n",
      "epoch 164: 0.9842588001983144 loss: 0.011586057022213936\n",
      "epoch 165: 0.9827329192546583 loss: 0.045441798865795135\n",
      "epoch 166: 0.9839367354503892 loss: 0.05224175006151199\n",
      "epoch 167: 0.9813648954923193 loss: 0.0700950101017952\n",
      "epoch 168: 0.9777165748622935 loss: 0.03185248002409935\n",
      "epoch 169: 0.9818659329664833 loss: 0.030727704986929893\n",
      "epoch 170: 0.9827197595792637 loss: 0.0465482622385025\n",
      "epoch 171: 0.9800526910048928 loss: 0.0352836437523365\n",
      "epoch 172: 0.9798155993022676 loss: 0.04451605677604675\n",
      "epoch 173: 0.9782963241751349 loss: 0.047339558601379395\n",
      "epoch 174: 0.9843632724543407 loss: 0.052982211112976074\n",
      "epoch 175: 0.9790359025859905 loss: 0.023144245147705078\n",
      "epoch 176: 0.9827759617802364 loss: 0.05408076196908951\n",
      "epoch 177: 0.9802574034736974 loss: 0.017078978940844536\n",
      "epoch 178: 0.9830614024162412 loss: 0.05895852670073509\n",
      "epoch 179: 0.9820057883478042 loss: 0.045756395906209946\n",
      "epoch 180: 0.9813142713819915 loss: 0.07973326742649078\n",
      "epoch 181: 0.9824211444956988 loss: 0.0546148419380188\n",
      "epoch 182: 0.9820238843494657 loss: 0.038581013679504395\n",
      "epoch 183: 0.9821720025109856 loss: 0.036484308540821075\n",
      "epoch 184: 0.9846591612025238 loss: 0.0545278899371624\n",
      "epoch 185: 0.9830145948666331 loss: 0.024710344150662422\n",
      "epoch 186: 0.9829113134588998 loss: 0.03355877846479416\n",
      "epoch 187: 0.9862258953168044 loss: 0.02626805566251278\n",
      "epoch 188: 0.9803921568627451 loss: 0.03417189046740532\n",
      "epoch 189: 0.9857933115413628 loss: 0.06152627617120743\n",
      "epoch 190: 0.9862572737402501 loss: 0.05523190647363663\n",
      "epoch 191: 0.9847148005467876 loss: 0.0687764436006546\n",
      "epoch 192: 0.982509122939474 loss: 0.040535818785429\n",
      "epoch 193: 0.9859836268915901 loss: 0.004167213570326567\n",
      "epoch 194: 0.9856855851330989 loss: 0.04041392728686333\n",
      "epoch 195: 0.9830191035085529 loss: 0.03041478432714939\n",
      "epoch 196: 0.9828406813627254 loss: 0.01572270132601261\n",
      "epoch 197: 0.9888985904951977 loss: 0.07811232656240463\n",
      "epoch 198: 0.9854763991486165 loss: 0.07631848007440567\n",
      "epoch 199: 0.9831165582791396 loss: 0.04274870827794075\n",
      "epoch 200: 0.9838527975966954 loss: 0.04134588688611984\n",
      "epoch 201: 0.9849605522682445 loss: 0.06163671612739563\n",
      "epoch 202: 0.9872524752475248 loss: 0.11561324447393417\n",
      "epoch 203: 0.9833438485804417 loss: 0.02678670920431614\n",
      "epoch 204: 0.9861214374225526 loss: 0.034778665751218796\n",
      "epoch 205: 0.9840712404364731 loss: 0.01779291033744812\n",
      "epoch 206: 0.9879368237781371 loss: 0.04700533673167229\n",
      "epoch 207: 0.9853069356244553 loss: 0.06062788516283035\n",
      "epoch 208: 0.9840059977508434 loss: 0.019809557124972343\n",
      "epoch 209: 0.9878139766227306 loss: 0.014827598817646503\n",
      "epoch 210: 0.9855036550613306 loss: 0.054084986448287964\n",
      "epoch 211: 0.9813826486285218 loss: 0.07884754985570908\n",
      "epoch 212: 0.9836855765665554 loss: 0.019330119714140892\n",
      "epoch 213: 0.9831382911790613 loss: 0.012789796106517315\n",
      "epoch 214: 0.9856639839034205 loss: 0.01322638988494873\n",
      "epoch 215: 0.9841779975278121 loss: 0.0757107138633728\n",
      "epoch 216: 0.9859701866466241 loss: 0.015170975588262081\n",
      "epoch 217: 0.9858732341542693 loss: 0.04093228280544281\n",
      "epoch 218: 0.9875508443239246 loss: 0.014540493488311768\n",
      "epoch 219: 0.9858437849248727 loss: 0.05651552230119705\n",
      "epoch 220: 0.985190670122177 loss: 0.041415076702833176\n",
      "epoch 221: 0.9869191478759187 loss: 0.01904076710343361\n",
      "epoch 222: 0.9841788046207935 loss: 0.047074053436517715\n",
      "epoch 223: 0.9896065330363771 loss: 0.018757537007331848\n",
      "epoch 224: 0.9868502667162883 loss: 0.022218216210603714\n",
      "epoch 225: 0.9887528117970508 loss: 0.0311126708984375\n",
      "epoch 226: 0.9841091091091091 loss: 0.049611300230026245\n",
      "epoch 227: 0.9871169480925579 loss: 0.012915678322315216\n",
      "epoch 228: 0.9857807977853278 loss: 0.011155555956065655\n",
      "epoch 229: 0.9835288245570252 loss: 0.08906475454568863\n",
      "epoch 230: 0.9832630795933548 loss: 0.0395168773829937\n",
      "epoch 231: 0.988050784167289 loss: 0.01789608784019947\n",
      "epoch 232: 0.9816167212289096 loss: 0.023707488551735878\n",
      "epoch 233: 0.9866433653726127 loss: 0.048155996948480606\n",
      "epoch 234: 0.9860535243120995 loss: 0.028948837891221046\n",
      "epoch 235: 0.9869719200607134 loss: 0.013185746036469936\n",
      "epoch 236: 0.9861791682372157 loss: 0.023995568975806236\n",
      "epoch 237: 0.9847491807411142 loss: 0.019336147233843803\n",
      "epoch 238: 0.9880668257756563 loss: 0.039681900292634964\n",
      "epoch 239: 0.9890096320079031 loss: 0.012214555405080318\n",
      "epoch 240: 0.9875784190715182 loss: 0.040586601942777634\n",
      "epoch 241: 0.983044508166064 loss: 0.03722032904624939\n",
      "epoch 242: 0.9842223891810669 loss: 0.027089862152934074\n",
      "epoch 243: 0.9857517810273716 loss: 0.01117475051432848\n",
      "epoch 244: 0.9836867862969005 loss: 0.034799396991729736\n",
      "epoch 245: 0.9848142570281124 loss: 0.04288594052195549\n",
      "epoch 246: 0.9876481597005614 loss: 0.057444971054792404\n",
      "epoch 247: 0.9858728557013118 loss: 0.04640578106045723\n",
      "epoch 248: 0.9873577749683944 loss: 0.006104385945945978\n",
      "epoch 249: 0.9875424688561721 loss: 0.009345898404717445\n",
      "epoch 250: 0.9886377824946935 loss: 0.03373872488737106\n",
      "epoch 251: 0.9862640762281896 loss: 0.029472021386027336\n",
      "epoch 252: 0.9864014102241249 loss: 0.030691727995872498\n",
      "epoch 253: 0.9876850354521707 loss: 0.0105460649356246\n",
      "epoch 254: 0.9873860372174348 loss: 0.047762177884578705\n",
      "epoch 255: 0.9876358186586737 loss: 0.033669788390398026\n",
      "epoch 256: 0.981360201511335 loss: 0.02134057879447937\n",
      "epoch 257: 0.9885372539247446 loss: 0.01863858662545681\n",
      "epoch 258: 0.9847472582881633 loss: 0.0419219546020031\n",
      "epoch 259: 0.9839438033115906 loss: 0.024117065593600273\n",
      "epoch 260: 0.9838220424671386 loss: 0.02913912944495678\n",
      "epoch 261: 0.9864304560874482 loss: 0.024440336972475052\n",
      "epoch 262: 0.9892286740126285 loss: 0.05217764526605606\n",
      "epoch 263: 0.984869325997249 loss: 0.05554533749818802\n",
      "epoch 264: 0.9891737182677949 loss: 0.011066831648349762\n",
      "epoch 265: 0.9899076923076923 loss: 0.0474502369761467\n",
      "epoch 266: 0.9880713209442491 loss: 0.023242441937327385\n",
      "epoch 267: 0.987759180614539 loss: 0.057047296315431595\n",
      "epoch 268: 0.9911581569115816 loss: 0.03765701875090599\n",
      "epoch 269: 0.985216072782411 loss: 0.031251054257154465\n",
      "epoch 270: 0.9898326100433974 loss: 0.06262581050395966\n",
      "epoch 271: 0.9872635561160151 loss: 0.028517980128526688\n",
      "epoch 272: 0.9873003897900163 loss: 0.07221421599388123\n",
      "epoch 273: 0.9882661340656597 loss: 0.029298117384314537\n",
      "epoch 274: 0.9890164752870694 loss: 0.0497661791741848\n",
      "epoch 275: 0.9867615836143374 loss: 0.00946421641856432\n",
      "epoch 276: 0.9886349286526076 loss: 0.09704677015542984\n",
      "epoch 277: 0.9849340866290018 loss: 0.010519127361476421\n",
      "epoch 278: 0.9865069356872636 loss: 0.009478298015892506\n",
      "epoch 279: 0.9912401451633087 loss: 0.032933615148067474\n",
      "epoch 280: 0.9860227130912268 loss: 0.029670709744095802\n",
      "epoch 281: 0.9885950620378493 loss: 0.027242595329880714\n",
      "epoch 282: 0.9887359198998749 loss: 0.00935549009591341\n",
      "epoch 283: 0.9868470033502916 loss: 0.020144490525126457\n",
      "epoch 284: 0.9897191574724172 loss: 0.016307486221194267\n",
      "epoch 285: 0.9886292640259903 loss: 0.02738536335527897\n",
      "epoch 286: 0.9865890688259109 loss: 0.034925516694784164\n",
      "epoch 287: 0.9893643334157803 loss: 0.031145978718996048\n",
      "epoch 288: 0.988810145468109 loss: 0.04395240917801857\n",
      "epoch 289: 0.9857886055069154 loss: 0.06665443629026413\n",
      "epoch 290: 0.9876497005988024 loss: 0.011400352232158184\n",
      "epoch 291: 0.9873641647712914 loss: 0.07604991644620895\n",
      "epoch 292: 0.9876934788124841 loss: 0.022103620693087578\n",
      "epoch 293: 0.9868142803831322 loss: 0.05658789351582527\n",
      "epoch 294: 0.9870308018456166 loss: 0.027594594284892082\n",
      "epoch 295: 0.9882220273148729 loss: 0.015511952340602875\n",
      "epoch 296: 0.9901753513244621 loss: 0.06567487865686417\n",
      "epoch 297: 0.989656912209889 loss: 0.01372982282191515\n",
      "epoch 298: 0.9891992551210428 loss: 0.01047415193170309\n",
      "epoch 299: 0.9914551083591331 loss: 0.09104038774967194\n",
      "epoch 300: 0.9886477045908184 loss: 0.01843247003853321\n",
      "epoch 301: 0.9860937108494112 loss: 0.01844663731753826\n",
      "epoch 302: 0.9897461548080531 loss: 0.024398861452937126\n",
      "epoch 303: 0.987012987012987 loss: 0.017703115940093994\n",
      "epoch 304: 0.9901324006994754 loss: 0.028539402410387993\n",
      "epoch 305: 0.9833903892481298 loss: 0.020693480968475342\n",
      "epoch 306: 0.9893114591101169 loss: 0.03377300873398781\n",
      "epoch 307: 0.9843809043960197 loss: 0.016337713226675987\n",
      "epoch 308: 0.9904988123515439 loss: 0.012679830193519592\n",
      "epoch 309: 0.9874671011404937 loss: 0.00629193801432848\n",
      "epoch 310: 0.988314271506713 loss: 0.010210055857896805\n",
      "epoch 311: 0.990627343164209 loss: 0.004178111907094717\n",
      "epoch 312: 0.9891783062790991 loss: 0.009307525120675564\n",
      "epoch 313: 0.9871362557761958 loss: 0.028898293152451515\n",
      "epoch 314: 0.9857849980962051 loss: 0.05917522311210632\n",
      "epoch 315: 0.9869933072357621 loss: 0.06083101034164429\n",
      "epoch 316: 0.9894219871552701 loss: 0.023183995857834816\n",
      "epoch 317: 0.9885 loss: 0.002405081642791629\n",
      "epoch 318: 0.9877722583110432 loss: 0.04560571536421776\n",
      "epoch 319: 0.9903010454717219 loss: 0.0024046713951975107\n",
      "epoch 320: 0.9872216599190283 loss: 0.04554424807429314\n",
      "epoch 321: 0.9897959183673469 loss: 0.017024096101522446\n",
      "epoch 322: 0.9888596820628364 loss: 0.0111891133710742\n",
      "epoch 323: 0.9912696433025692 loss: 0.05342615768313408\n",
      "epoch 324: 0.987277036297867 loss: 0.023785412311553955\n",
      "epoch 325: 0.9881086493929152 loss: 0.02692830003798008\n",
      "epoch 326: 0.990316573556797 loss: 0.04579069837927818\n",
      "epoch 327: 0.9870619268936063 loss: 0.012578579597175121\n",
      "epoch 328: 0.988485607008761 loss: 0.032521575689315796\n",
      "epoch 329: 0.9923076923076923 loss: 0.046314019709825516\n",
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "MAX_DELAY = 8\n",
    "SEQ_LENGTH = 20\n",
    "\n",
    "model = VariableDelayGRUMemory(hidden_size=512, max_delay=MAX_DELAY).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[100, 200, 300], gamma=0.8)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.5)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "epoch = 0\n",
    "test_acc = 0\n",
    "while test_acc <= 0.992:\n",
    "    dataset = VariableDelayEchoDataset(max_delay=8, seq_length=20, size=2000)\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
    "    for x, delay, y in train_dataloader:\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        logits = model(x, delay)\n",
    "        \n",
    "        mask = [torch.zeros(d) for d in delay]\n",
    "        mask.append(torch.zeros(SEQ_LENGTH))\n",
    "        mask = torch.nn.utils.rnn.pad_sequence(mask, padding_value=1, batch_first=True)[:-1]\n",
    "        logits = logits.reshape(-1, N + 1)\n",
    "            \n",
    "        loss = criterion(logits, y.reshape(-1))\n",
    "        loss = loss.reshape(batch_size, SEQ_LENGTH)\n",
    "        loss = torch.mean(loss * mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.0005)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    test_acc = test_variable_delay_model(model)\n",
    "    # scheduler.step()\n",
    "    print(f'epoch {epoch}: {test_acc}', f'loss: {loss.item()}')\n",
    "    epoch += 1\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "accuracy = test_variable_delay_model(model)\n",
    "assert duration < 600, 'execution took f{duration:.2f} seconds, which longer than 10 mins'\n",
    "assert accuracy > 0.99, f'accuracy is too low, got {accuracy}, need 0.99'\n",
    "print('tests passed')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
